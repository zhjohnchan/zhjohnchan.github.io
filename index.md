---
layout: homepage
---

## About Me

Hi, I am a third-year Ph.D. student at The Chinese University of Hong Kong, Shenzhen. I am supervised jointly by Prof. [Xiang Wan](https://scholar.google.com/citations?user=e3_kWigAAAAJ&hl=en&oi=ao) and Prof. [Guanbin Li](http://guanbinli.com/). I also work closely with Prof. [Benyou Wang](https://wabyking.github.io/old.html). I received my B.Eng. degree from Sun Yat-sen University in 2019, and then became a Ph.D. student at CUHK(SZ). My research lies at the intersection of **vision+language** and **large-scale generative models**. I am always open to collaboration. Feel free to drop me an e-mail. :-)

<strong style="color:#e74d3c; font-weight:600">I am looking for an intern/postdoctoral position on **Vision+Language** and **Large Language Models**. I'd appreciate a ping if you see a job I might fit.</strong>

## Research Interests

- **Vision+Language**
- **Large Language Models** 

## News
- **[May  2023]** Three papers are accepted to ACL 2023.
- **[Apr. 2023]** We are excited to release LLMZoo, including the `Phoenix` and `Chimera` models.
- **[Mar. 2023]** One paper is accepted to CVPR 2023
- **[Nov. 2022]** Two papers are accepted to AAAI 2023.
- **[Jul. 2022]** One paper is accepted to ACMMM 2022.
- **[Apr. 2022]** One paper is accepted to MICCAI 2022.
- **[Feb. 2022]** One paper is accepted to ACL 2022.

## Project

- **LLM Zoo: democratizing ChatGPT**
  <br>
  **Project Leader**
  <br>
  [[Project Website]](https://github.com/FreedomIntelligence/LLMZoo)

## Publications

- **Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge**
  <br>
  **Zhihong Chen**, Guanbin Li and Xiang Wan
  <br>
  **ACMMM 2022**.
  <br>
  [[PDF](https://arxiv.org/pdf/2209.07118.pdf)] [[CODE](https://github.com/zhjohnchan/ARL)]

- **Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training**
  <br>
  **Zhihong Chen**, Yuhao Du, Jinpeng Hu, Yang Liu, Guanbin Li, Xiang Wan and Tsung-Hui Chang
  <br>
  **MICCAI 2022**.
  <br>
  [[PDF](https://arxiv.org/pdf/2209.07098.pdf)] [[CODE](https://github.com/zhjohnchan/M3AE)]

- **Graph Enhanced Contrastive Learning for Radiology Findings Summarization**
  <br>
  Jinpeng Hu, Zhuo Li, **Zhihong Chen**, Zhen Li, Xiang Wan and Tsung-Hui Chang
  <br>
  **ACL 2022**.
  <br>
  [[PDF](https://arxiv.org/pdf/2204.00203.pdf)] [[CODE](https://github.com/jinpeng01/AIG_CL)]

- **Cross-modal Memory Networks for Radiology Report Generation**
  <br>
  **Zhihong Chen**, Yaling Shen, Yan Song and Xiang Wan
  <br>
  **ACL 2021**.
  <br>
  [[PDF](https://arxiv.org/pdf/2204.13258.pdf)] [[CODE](https://github.com/zhjohnchan/R2GenCMN)]

- **Word Graph Guided Summarization for Radiology Findings**
  <br>
  Jinpeng Hu, Jianling Li, **Zhihong Chen**, Yaling Shen, Yan Song, Xiang Wan and Tsung-Hui Chang 
  <br>
  **Findings of ACL 2021**.
  <br>
  [[PDF](https://arxiv.org/pdf/2112.09925.pdf)] [[CODE](https://github.com/jinpeng01/WGSum)]

- **Generating Radiology Reports via Memory-driven Transformer**
  <br>
  **Zhihong Chen**, Yan Song, Tsung-Hui Chang and Xiang Wan.
  <br>
  **EMNLP 2020**.
  <br>
  [[PDF](https://arxiv.org/pdf/2010.16056.pdf)] [[CODE](https://github.com/zhjohnchan/R2Gen)]

- **Hierarchical Attention Network for Image Captioning**
  <br>
  Weixuan Wang, **Zhihong Chen** and Haifeng Hu
  <br>
  **AAAI 2019**.
  <br>
  [[PDF](https://ojs.aaai.org/index.php/AAAI/article/download/4924/4797)] <strong><i style="color:#e74d3c">Spotlight</i></strong>

## Services

- Reviewers: ACL (2021; 2022; 2023), EMNLP (2020; 2022), NAACL (2022), COLING (2020), ARR (2021; 2022; 2023).

## About the Profile Photo
The profile photo was generated when I was a year-three undergraduate student and just starting to learn about deep learning. My buddy Aobo Yu and I worked on a *sketch colorization* course project ([slides](https://github.com/zhjohnchan/zhjohnchan.github.io/tree/master/assets/files/dip_slides.pdf) at that time, haha) based on [StarGAN](https://github.com/yunjey/stargan). We drew the cartoon sketch by hand and colorized it automatically.
